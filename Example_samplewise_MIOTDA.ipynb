{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example sample-wise adaptation using OTDA and BOTDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import ot\n",
    "import scipy.io\n",
    "import mne          \n",
    "from mne.decoding import CSP\n",
    "mne.set_log_level(verbose='warning') #to avoid info at terminal\n",
    "import matplotlib.pyplot as pl\n",
    "np.random.seed(100)\n",
    "from MIOTDAfunctions import*\n",
    "\n",
    "# get the functions from RPA package\n",
    "import rpa.transfer_learning as TL\n",
    "# pyriemann import\n",
    "from pyriemann.classification import MDM\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.utils.base import invsqrtm\n",
    "import timeit\n",
    "\n",
    "#ignore warning \n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data and filter it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fName = 'Data/DataSession1_S9.mat'\n",
    "s = scipy.io.loadmat(fName)\n",
    "\n",
    "Data_S1=s[\"X\"]\n",
    "Labels_S1=s[\"y\"]\n",
    "Labels_S1=np.squeeze(Labels_S1)\n",
    "\n",
    "#filterting with mne\n",
    "[nt, nc, ns]=np.shape(Data_S1)\n",
    "Data_S1=np.reshape(Data_S1, [nt, nc*ns])\n",
    "Data_S1=mne.filter.filter_data(Data_S1, 128, 8, 30)\n",
    "Data_S1=np.reshape(Data_S1, [nt,nc,ns])\n",
    "\n",
    "fName = 'Data/DataSession2_S9.mat'\n",
    "s2 = scipy.io.loadmat(fName)\n",
    "\n",
    "Data_S2=s2[\"X\"]\n",
    "Labels_S2=s2[\"y\"]\n",
    "Labels_S2=np.squeeze(Labels_S2)\n",
    "\n",
    "#filterting with mne\n",
    "[nt, nc, ns]=np.shape(Data_S2)\n",
    "Data_S2=np.reshape(Data_S2, [nt, nc*ns])\n",
    "Data_S2=mne.filter.filter_data(Data_S2, 128, 8, 30)\n",
    "Data_S2=np.reshape(Data_S2, [nt,nc,ns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learn CSP+LDA from source data (Data_S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr=Data_S1\n",
    "Ytr=Labels_S1\n",
    "csp = CSP(n_components=6, reg='empirical', log=True, norm_trace=False, cov_est='epoch')\n",
    "#learn csp filters\n",
    "Gtr=csp.fit_transform(Xtr,Ytr)\n",
    "#learn lda\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(Gtr,Ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first 20 trials of the new session used as transportation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "Labels_te=Labels_S2[20:]\n",
    "##\n",
    "Xval=Data_S2[0:20]\n",
    "Yval=Labels_S2[0:20]\n",
    "##\n",
    "Gval=csp.transform(Xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for saving outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_predict_sc=[]\n",
    "yt_predict_sr=[]\n",
    "yt_predict_1=[]\n",
    "yt_predict_2=[]\n",
    "yt_predict_3=[]\n",
    "yt_predict_4=[]\n",
    "yt_predict_rpa=[]\n",
    "yt_predict_eu=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set OTDA params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rango_cl=[0.1, 1, 10]\n",
    "rango_e=[0.1, 1, 10] \n",
    "metrica = 'sqeuclidean'\n",
    "outerkfold = 20\n",
    "innerkfold = None\n",
    "M=20\n",
    "clf=LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select subset Gtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset selection re-training path\n",
    "G_FOTDAs_, Y_FOTDAs_, regu_FOTDAs_=\\\n",
    "SelectSubsetTraining_OTDAs(Gtr, Ytr, Gval, Yval, rango_e, clf, metrica, outerkfold, innerkfold, M)\n",
    "G_FOTDAl1l2_, Y_FOTDAl1l2_, regu_FOTDAl1l2_=\\\n",
    "    SelectSubsetTraining_OTDAl1l2(Gtr, Ytr, Gval, Yval, rango_e, rango_cl, clf, metrica, outerkfold, innerkfold, M)\n",
    "G_BOTDAs_, Y_BOTDAs_, regu_BOTDAs_=\\\n",
    "SelectSubsetTraining_BOTDAs(Gtr, Ytr, Gval, Yval, rango_e, lda, metrica, outerkfold, innerkfold, M)\n",
    "G_BOTDAl1l2_, Y_BOTDAl1l2_, regu_BOTDAl1l2_=\\\n",
    "SelectSubsetTraining_BOTDAl1l2(Gtr, Ytr, Gval, Yval, rango_e, rango_cl, lda, metrica, outerkfold, innerkfold, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for each trial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running testing trial=10\n",
      "Running testing trial=20\n",
      "Running testing trial=30\n",
      "Running testing trial=40\n",
      "Running testing trial=50\n",
      "Running testing trial=60\n"
     ]
    }
   ],
   "source": [
    "for re in range(1,len(Labels_te)+1):\n",
    "    if np.mod(re,10)==0 : print('Running testing trial={:1.0f}'.format(re))\n",
    "    #testing trial\n",
    "    Xte=Data_S2[20+(re-1):20+(re)]\n",
    "    Xte=Xte.reshape(1, nc, ns)\n",
    "    Yte=Labels_S2[20+(re-1):20+(re)]\n",
    "    \n",
    "    Xval=np.vstack((Xval, Xte))\n",
    "    Yval=np.hstack((Yval, Yte))\n",
    "    \n",
    "    #csp estimation\n",
    "    Gval=csp.transform(Xval)\n",
    "    Gte=csp.transform(Xte)\n",
    "    #feature computation\n",
    "    Gte=csp.transform(Xte)\n",
    "    \n",
    "    #evaluate SC  \n",
    "    yt_predict_sc.append(lda.predict(Gte))\n",
    "\n",
    "    #evaluate SR\n",
    "    # time\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    Xtr2=np.vstack((Xtr,Xval))\n",
    "    Ytr2=np.hstack((Ytr, Yval))\n",
    "    Xtr2=Xtr2[len(Yval):]\n",
    "    Ytr2=Ytr2[len(Yval):]\n",
    "\n",
    "    csp2 = CSP(n_components=6, reg='empirical', log=True, norm_trace=False, cov_est='epoch')\n",
    "    #learn csp filters\n",
    "    Gtr2=csp2.fit_transform(Xtr2,Ytr2)\n",
    "    #learn lda\n",
    "    lda2 = LinearDiscriminantAnalysis()\n",
    "    lda2.fit(Gtr2,Ytr2)\n",
    "\n",
    "    Gte2=csp2.transform(Xte)\n",
    "\n",
    "    #ldatest\n",
    "    yt_predict_sr.append(lda2.predict(Gte2))\n",
    "    # time\n",
    "    stop = timeit.default_timer()\n",
    "    time_sr = stop - start\n",
    "    \n",
    "    #%% # Sinkhorn Transport\n",
    "    # time\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    Gtr_daot=G_FOTDAs_\n",
    "    Ytr_daot=Y_FOTDAs_ \n",
    "    ot_sinkhorn= ot.da.SinkhornTransport(metric=metrica,reg_e=regu_FOTDAs_)\n",
    "    #learn the map\n",
    "    ot_sinkhorn.fit(Xs=Gtr_daot, ys=Ytr_daot, Xt=Gval)\n",
    "    #apply the mapping over source data\n",
    "    transp_Xs_sinkhorn = ot_sinkhorn.transform(Xs=Gtr)\n",
    "\n",
    "    # retraining\n",
    "    lda3 = LinearDiscriminantAnalysis()\n",
    "    lda3.fit(transp_Xs_sinkhorn,Ytr)\n",
    "    # Compute acc\n",
    "    yt_predict_1.append(lda3.predict(Gte))\n",
    "    # time\n",
    "    stop = timeit.default_timer()\n",
    "    time_fs = stop - start\n",
    "\n",
    "    #%% # Group-Lasso Transport\n",
    "    # time\n",
    "    start = timeit.default_timer()\n",
    "    Gtr_daot=G_FOTDAl1l2_\n",
    "    Ytr_daot=Y_FOTDAl1l2_\n",
    "\n",
    "    ot_l1l2 = ot.da.SinkhornL1l2Transport(metric=metrica,reg_e=regu_FOTDAl1l2_[0], reg_cl=regu_FOTDAl1l2_[1])\n",
    "\n",
    "    ot_l1l2.fit(Xs=Gtr_daot, ys=Ytr_daot, Xt=Gval)\n",
    "\n",
    "    #transport taget samples onto source samples\n",
    "    transp_Xs_l1l2=ot_l1l2.transform(Xs=Gtr)\n",
    "\n",
    "    # retraining\n",
    "    lda3 = LinearDiscriminantAnalysis()\n",
    "    lda3.fit(transp_Xs_l1l2,Ytr)\n",
    "\n",
    "    # Compute acc\n",
    "    yt_predict_2.append(lda3.predict(Gte))\n",
    "    # time\n",
    "    stop = timeit.default_timer()\n",
    "    time_fg = stop - start\n",
    "\n",
    "    #%% # Backward Sinkhorn Transport\n",
    "    # time\n",
    "    start = timeit.default_timer()   \n",
    "    Gtr_botda=G_BOTDAs_\n",
    "    Ytr_botda=Y_BOTDAs_\n",
    "\n",
    "    bot_s = ot.da.SinkhornTransport(metric=metrica,reg_e=regu_BOTDAs_)\n",
    "\n",
    "\n",
    "    bot_s.fit(Xs=Gval, ys=Yval, Xt=Gtr_botda)\n",
    "    #transport testing samples\n",
    "    transp_Xt_s_backward=bot_s.transform(Xs=Gte)\n",
    "    # Compute accuracy one-training    \n",
    "    yt_predict_3.append(lda.predict(transp_Xt_s_backward))\n",
    "    # time\n",
    "    stop = timeit.default_timer()\n",
    "    time_bs = stop - start\n",
    "\n",
    "    #%% # Backward Group-Lasso Transport\n",
    "    # time\n",
    "    start = timeit.default_timer()    \n",
    "    Gtr_botda=G_BOTDAl1l2_\n",
    "    Ytr_botda=Y_BOTDAl1l2_\n",
    "\n",
    "    bot_l1l2 = ot.da.SinkhornL1l2Transport(metric=metrica,reg_e=regu_BOTDAl1l2_[0], reg_cl=regu_BOTDAl1l2_[1])\n",
    "\n",
    "    bot_l1l2.fit(Xs=Gval, ys=Yval, Xt=Gtr_botda)\n",
    "    #transport testing samples\n",
    "    transp_Xt_l1l2_backward=bot_l1l2.transform(Xs=Gte)\n",
    "    # Compute accuracy one-training    \n",
    "    yt_predict_4.append(lda.predict(transp_Xt_l1l2_backward))\n",
    "    # time\n",
    "    stop = timeit.default_timer()\n",
    "    time_bg = stop - start\n",
    "\n",
    "\n",
    "    \n",
    "    #%% # Riemann\n",
    "    # time\n",
    "    start_rpa = timeit.default_timer()\n",
    "    # cov matrix estimation\n",
    "    cov_tr = Covariances().transform(Xtr)\n",
    "    cov_val= Covariances().transform(Xval)\n",
    "    cov_te = Covariances().transform(Xte)\n",
    "        \n",
    "    clf = MDM()\n",
    "    source={'covs':cov_tr, 'labels': Ytr}\n",
    "    target_org_train={'covs':cov_val, 'labels': Yval}\n",
    "    target_org_test={'covs':cov_te, 'labels': Yte}\n",
    "    # get the score with the re-centered matrices\n",
    "    source_rct, target_rct_train, target_rct_test = TL.RPA_recenter(source, target_org_train, target_org_test)   \n",
    "    # rotate the re-centered-stretched matrices using information from classes\n",
    "    source_rpa, target_rpa_train, target_rpa_test = TL.RPA_rotate(source_rct, target_rct_train, target_rct_test)\n",
    "    # get score\n",
    "    covs_source, y_source = source_rpa['covs'], source_rpa['labels']\n",
    "    covs_target_train, y_target_train = target_rpa_train['covs'], target_rpa_train['labels']\n",
    "    covs_target_test, y_target_test = target_rpa_test['covs'], target_rpa_test['labels']\n",
    "    \n",
    "    covs_train = np.concatenate([covs_source, covs_target_train])\n",
    "    y_train = np.concatenate([y_source, y_target_train])\n",
    "    clf.fit(covs_train, y_train)\n",
    "\n",
    "    covs_test = covs_target_test\n",
    "    y_test = y_target_test\n",
    "\n",
    "    yt_predict_rpa.append(clf.predict(covs_test))\n",
    "    # time\n",
    "    stop_rpa = timeit.default_timer()\n",
    "    time_rpa = stop_rpa - start_rpa\n",
    "        \n",
    "    #%% # Euclidean\n",
    "    # get arithmetic mean\n",
    "    # time\n",
    "    start_eu = timeit.default_timer()\n",
    "    # Estimate single trial covariance\n",
    "    cov_tr = Covariances().transform(Xtr)\n",
    "    cov_val= Covariances().transform(Xval)\n",
    "    Ctr = cov_tr.mean(0)\n",
    "    Cval = cov_val.mean(0)\n",
    "    # aligment\n",
    "    Xtr_eu = np.asarray([np.dot(invsqrtm(Ctr), epoch) for epoch in Xtr])\n",
    "    Xval_eu = np.asarray([np.dot(invsqrtm(Cval), epoch) for epoch in Xval])\n",
    "    Xte_eu = np.asarray([np.dot(invsqrtm(Cval), epoch) for epoch in Xte])\n",
    "\n",
    "\n",
    "    x_train = np.concatenate([Xtr_eu, Xval_eu])\n",
    "    y_train = np.concatenate([Ytr, Yval])\n",
    "\n",
    "    # train new csp+lda\n",
    "\n",
    "    csp2 = CSP(n_components=6, reg='empirical', log=True, norm_trace=False, cov_est='epoch')\n",
    "    # learn csp filters\n",
    "    Gtr2 = csp2.fit_transform(x_train,y_train)\n",
    "    # learn lda\n",
    "    lda2=LinearDiscriminantAnalysis()\n",
    "\n",
    "    # lda2=LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "    lda2.fit(Gtr2,y_train)\n",
    "\n",
    "    Gte2=csp2.transform(Xte_eu)\n",
    "\n",
    "    # ldatest\n",
    "    yt_predict_eu.append(lda2.predict(Gte2))\n",
    "    # time\n",
    "    stop_eu= timeit.default_timer()\n",
    "    time_eu = stop_eu - start_eu\n",
    "    \n",
    "    #save times\n",
    "    times = [time_sr, time_rpa, time_eu,time_fs, time_fg, time_bs, time_bg]\n",
    "        \n",
    "    if re==1:\n",
    "        times_se = times\n",
    "    else:\n",
    "        times_se = np.vstack((times_se, times))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy \n",
    "yt_predict_4=np.squeeze(np.asarray(yt_predict_4))\n",
    "yt_predict_3=np.squeeze(np.asarray(yt_predict_3))\n",
    "yt_predict_2=np.squeeze(np.asarray(yt_predict_2))\n",
    "yt_predict_1=np.squeeze(np.asarray(yt_predict_1))\n",
    "yt_predict_sc=np.squeeze(np.asarray(yt_predict_sc))\n",
    "yt_predict_sr=np.squeeze(np.asarray(yt_predict_sr))\n",
    "\n",
    "acc_botdal1l2=accuracy_score(Labels_te, yt_predict_4)\n",
    "acc_botdas=accuracy_score(Labels_te, yt_predict_3)\n",
    "acc_fotdal1l2=accuracy_score(Labels_te, yt_predict_2)\n",
    "acc_fotdas=accuracy_score(Labels_te, yt_predict_1)\n",
    "acc_sc=accuracy_score(Labels_te, yt_predict_sc)\n",
    "acc_sr=accuracy_score(Labels_te, yt_predict_sr)\n",
    "\n",
    "#print accuracy\n",
    "acc={}\n",
    "acc[\"sc\"]=acc_sc\n",
    "acc[\"sr\"]=acc_sr\n",
    "acc[\"fotda_s\"]=acc_fotdas\n",
    "acc[\"fotda_l1l2\"]=acc_fotdal1l2\n",
    "acc[\"botda_s\"]=acc_botdas\n",
    "acc[\"botda_l1l2\"]=acc_botdal1l2\n",
    "    \n",
    "print(acc)  \n",
    "\n",
    "#print computing time\n",
    "mean_time = times.mean(axis=0)\n",
    "time = {}\n",
    "time[\"sr\"] = round(mean_time[0],3)\n",
    "time[\"rpa\"] = round(mean_time[1],3)\n",
    "time[\"eu\"] = round(mean_time[2],3)\n",
    "time[\"fotda_s\"] = round(mean_time[3],3)\n",
    "time[\"fotda_l1l2\"] = round(mean_time[4],3)\n",
    "time[\"botda_s\"] = round(mean_time[5],3)\n",
    "time[\"botda_l1l2\"] = round(mean_time[6],3)\n",
    "    \n",
    "print(time)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
